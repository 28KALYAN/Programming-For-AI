# Core libraries
import pandas as pd
import numpy as np

# NLP libraries
import nltk
import re
import string

# Machine Learning
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC

# Evaluation
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score

# Visualisation
import matplotlib.pyplot as plt
import seaborn as sns

# NLTK downloads
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('punkt_tab')
nltk.download('wordnet')

# Load datasets
fake = pd.read_csv("Fake.csv")
true = pd.read_csv("True.csv")

# Add labels
fake["label"] = 0   # Fake news
true["label"] = 1   # Real news

# Combine datasets
df = pd.concat([fake, true], axis=0)

# Shuffle dataset
df = df.sample(frac=1).reset_index(drop=True)

# Inspect dataset
df.head()

# Dataset shape
print("Dataset shape:")
df.shape

# Check missing values
print("Missing values:")
df.isnull().sum()

# Data visualisations

# Subject-wise count
subject_counts = df['subject'].value_counts()

plt.figure(figsize=(8,5))
sns.barplot(
    x=subject_counts.values,
    y=subject_counts.index
)
plt.title('Distribution of News Articles by Subject')
plt.xlabel('Number of Articles')
plt.ylabel('News Subject')
plt.show()

# Count of each class
class_counts = df['label'].value_counts()

# Labels
labels = ['Fake News', 'Real News']

plt.figure(figsize=(6,6))
plt.pie(
    class_counts,
    labels=labels,
    autopct='%1.1f%%',
    startangle=90,
    explode=(0.05, 0.05)
)
plt.title('Proportion of Fake and Real News Articles')
plt.axis('equal')
plt.show()

# Text Cleaning & Preprocessing
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def preprocess_text(text):
    # Convert to lowercase
    text = text.lower()

    # Remove URLs
    text = re.sub(r"http\S+|www\S+|https\S+", "", text)

    # Remove punctuation and numbers
    text = re.sub(r"[^a-z\s]", "", text)

    # Tokenisation
    tokens = nltk.word_tokenize(text)

    # Remove stopwords and lemmatise
    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]

    return " ".join(tokens)

# Apply preprocessing on article text
df["clean_text"] = df["text"].apply(preprocess_text)

df[["text", "clean_text"]].head()

# Train–Test Split
X = df["clean_text"]
y = df["label"]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Feature Extraction using TF-IDF
tfidf = TfidfVectorizer(
    max_df=0.7,
    min_df=5,
    ngram_range=(1,2)
)

X_train_tfidf = tfidf.fit_transform(X_train)
X_test_tfidf = tfidf.transform(X_test)

X_train_tfidf.shape

# Rule-Based Fake News Classifier
fake_keywords = [
    "breaking", "shocking", "you wont believe", "exposed",
    "truth revealed", "hoax", "conspiracy"
]

def rule_based_classifier(text):
    score = 0
    for word in fake_keywords:
        if word in text:
            score += 1
    return 0 if score > 0 else 1   # 0 = Fake, 1 = Real

# Apply rule-based predictions
rule_preds = X_test.apply(rule_based_classifier)

# Evaluate
print("Rule-Based Accuracy:", accuracy_score(y_test, rule_preds))
print(classification_report(y_test, rule_preds))

# Machine Learning Models

# Naive Bayes
nb = MultinomialNB()
nb.fit(X_train_tfidf, y_train)

nb_preds = nb.predict(X_test_tfidf)

print("Naive Bayes Accuracy:", accuracy_score(y_test, nb_preds))
print(classification_report(y_test, nb_preds))

# Logistic Regression
lr = LogisticRegression(max_iter=1000)
lr.fit(X_train_tfidf, y_train)

lr_preds = lr.predict(X_test_tfidf)

print("Logistic Regression Accuracy:", accuracy_score(y_test, lr_preds))
print(classification_report(y_test, lr_preds))

# Support Vector Machine (SVM)
svm = LinearSVC()
svm.fit(X_train_tfidf, y_train)

svm_preds = svm.predict(X_test_tfidf)

print("SVM Accuracy:", accuracy_score(y_test, svm_preds))
print(classification_report(y_test, svm_preds))

# Confusion Matrix Visualisation

# Rule-based prediction
cm = confusion_matrix(y_test, rule_preds)

plt.figure(figsize=(6,4))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix – Naive Bayes")
plt.show()

# Naive Bayes
cm = confusion_matrix(y_test, nb_preds)

plt.figure(figsize=(6,4))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix – Rule-based prediction")
plt.show()

# Logistic Regression
cm = confusion_matrix(y_test, lr_preds)

plt.figure(figsize=(6,4))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix – Logistic Regression")
plt.show()

# Support Vector Machine (SVM)
cm = confusion_matrix(y_test, svm_preds)

plt.figure(figsize=(6,4))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix – SVM")
plt.show()

# Generate scores for all models

def rule_based_score(text):
    score = 0
    for word in fake_keywords:
        if word in text:
            score += 1
    return score

# Apply rule-based scores
rule_scores = X_test.apply(rule_based_score)

# Normalise scores between 0 and 1
rule_scores_norm = rule_scores / rule_scores.max()

# Naive Bayes - probabilities
nb_probs = nb.predict_proba(X_test_tfidf)[:, 1]

# Logistic Regression - probabilities
lr_probs = lr.predict_proba(X_test_tfidf)[:, 1]

# SVM – Decision Function
svm_scores = svm.decision_function(X_test_tfidf)

# Compute ROC Curves & AUC Scores
from sklearn.metrics import roc_curve, roc_auc_score

# Rule-based ROC (invert since higher score = fake)
fpr_rule, tpr_rule, _ = roc_curve(y_test, 1 - rule_scores_norm)
auc_rule = roc_auc_score(y_test, 1 - rule_scores_norm)

# Naive Bayes ROC
fpr_nb, tpr_nb, _ = roc_curve(y_test, nb_probs)
auc_nb = roc_auc_score(y_test, nb_probs)

# Logistic Regression ROC
fpr_lr, tpr_lr, _ = roc_curve(y_test, lr_probs)
auc_lr = roc_auc_score(y_test, lr_probs)

# SVM ROC
fpr_svm, tpr_svm, _ = roc_curve(y_test, svm_scores)
auc_svm = roc_auc_score(y_test, svm_scores)

# Plot Combined ROC Curve (ALL MODELS)
plt.figure(figsize=(8,6))

plt.plot(fpr_rule, tpr_rule, linestyle='--', label=f'Rule-Based (AUC = {auc_rule:.3f})')
plt.plot(fpr_nb, tpr_nb, label=f'Naive Bayes (AUC = {auc_nb:.3f})')
plt.plot(fpr_lr, tpr_lr, label=f'Logistic Regression (AUC = {auc_lr:.3f})')
plt.plot(fpr_svm, tpr_svm, label=f'SVM (AUC = {auc_svm:.3f})')

plt.plot([0,1], [0,1], 'k--', label='Chance-level (AUC = 0.5)')

plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve Comparison – Fake News Detection Models')
plt.legend(loc='lower right')
plt.grid(True)
plt.show()